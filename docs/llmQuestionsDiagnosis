
# llm - Ordner

# llmService
This node.js-module serves to establish communication with ollama (lokal llm). Here the model mistral-instruct is used, which usually performs better in dialogue and instruction-following compared to the normal mistral model. It is necessary to pull this model in order to use it. This model uses axios to make HTTP requests to the local Ollama-API

needed Installations:

install node.js: https://nodejs.org/en

npm install axios --> install node.js module 'axios'

install ollama via website: https://ollama.com/download

ollama pull mistral:instruct  --> pull model

ollama run mistral:instruct  --> run model


# llmQuestionsDiagnosis - Question and Diagnosis Generator
This module describes two functions which interact with a local LLM (Ollama).
1. getFollowUpQuestions(symptoms, previousAnswer): This function generates a medical follow up question using Ollama according to the reported symptoms and previous answers. 
2. getDiagnosis(symptoms, answers): This function returns a possible medical diagnosis and a recommendation according to the symptoms and given answers

To get the questions, diagnosis and recommendation these functions use a prompt directed to the local Ollama API using 'axios'. 

# llmTest - testFile to test the two functions in llmQuestionsDiagnosis 
Test question generation and diagnosis/recommendation generation


# extractSymptoms - Extraction of Symptoms from a text 
This node.js module also uses a prompt directed to ollama to extract medical symptoms from a text. The return is an array of distinct symptoms in JSON. 

# testExtractSymptoms - test file to test extractSymptoms - module 
test extraction of symptoms from a defined text

# splitDiagnosisRecommendation
this module defines one function which uses Regex to locate and extract the diagnosis and recommendation from a text. The return values are the seperated values of diagnosis and recommendation 















